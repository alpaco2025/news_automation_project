import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime
import re
import json

BASE_URL = "https://www.aitimes.com"


# ---------------------------------------------------------
# ë³´ê¸° ì¢‹ì€ ì¶œë ¥
# ---------------------------------------------------------
def pretty_print_article(article):
    print("\n" + "=" * 80)
    print(f"ğŸ“° ì œëª©: {article['title']}")
    print(f"ğŸ“‚ ì¹´í…Œê³ ë¦¬: {article['category']}")
    print(f"â± ë‚ ì§œ: {article['article_date']}")
    print(f"ğŸ”— URL: {article['url']}")
    print("-" * 80)
    print("ğŸ“ ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸° (ì• 200ì):")
    print(article["content"][:200] + "...")
    print("=" * 80 + "\n")


# ---------------------------------------------------------
# ìœ ë‹ˆì½”ë“œ ì œê±°
# ---------------------------------------------------------
def clean_invisible_chars(text):
    if not text:
        return ""
    for ch in ["\u200b", "\ufeff", "\u2060", "\u180e"]:
        text = text.replace(ch, "")
    return text.strip()


# ---------------------------------------------------------
# ë‚ ì§œ íŒŒì‹±
# ---------------------------------------------------------
def extract_korean_date(text):
    if not text:
        return None

    text = text.replace("ì…ë ¥", "").strip()
    match = re.search(r"\d{4}\.\d{2}\.\d{2} \d{2}:\d{2}", text)

    if not match:
        return None

    try:
        return datetime.strptime(match.group(), "%Y.%m.%d %H:%M")
    except:
        return None


# ---------------------------------------------------------
# ì œëª©(title) ì •ë¦¬
# ---------------------------------------------------------
def clean_title(text):
    if not text:
        return ""

    text = text.replace("â€œ", "\"").replace("â€", "\"")
    text = text.replace("â€˜", "'").replace("â€™", "'")

    # ì•ìª½ [ë¸Œë¼ì¼“] ì œê±°
    text = re.sub(r"^\[[^\]]+\]\s*", "", text)

    # ì œëª© ì•ë’¤ì˜ ê³¼ë„í•œ ë”°ì˜´í‘œ ì œê±°
    text = re.sub(r'^"+', '', text)
    text = re.sub(r'"+$', '', text)

    # ë‚´ë¶€ ëˆ„ì  ë”°ì˜´í‘œ ì •ë¦¬
    text = re.sub(r'""+', '"', text)

    return text.strip()


# ---------------------------------------------------------
# ë³¸ë¬¸(content) ì „ì²˜ë¦¬ â€“ ì „ì²´ ê°•í™”
# ---------------------------------------------------------
def clean_article_body(text):
    if not text:
        return ""

    # 1) ì„œë¡ /ê´‘ê³  ì œê±°
    remove_patterns = [
        r"ê¸°ì‚¬ë¥¼\s*ì½ì–´ë“œë¦½ë‹ˆë‹¤\.?",
        r"AIíƒ€ì„ìŠ¤ì…ë‹ˆë‹¤\.?",
        r"ì´\s*ë‰´ìŠ¤ëŠ”\s*AIê°€\s*ì½ì–´ë“œë¦½ë‹ˆë‹¤\.?",
        r"\(ì‚¬ì§„\s*=\s*[^)]+\)",
        r"\(ê·¸ë˜í”½\s*=\s*[^)]+\)",
    ]
    for p in remove_patterns:
        text = re.sub(p, "", text)

    # 2) ê¸°ì + ë©”ì¼ ì œê±°
    text = re.sub(r"[ê°€-í£]{2,10}\s*[A-Za-z0-9._%+-]+\s*\.\s*[A-Za-z]{2,4}", "", text)
    text = re.sub(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+", "", text)
    text = re.sub(r"(ê¸°ì|íŠ¹íŒŒì›)", "", text)

    # 3) ì´ì–´ XXì¼ ì£¼ìš” ë‰´ìŠ¤ì…ë‹ˆë‹¤ ì œê±°
    text = re.split(r"ì´ì–´\s*\d+ì¼\s*ì£¼ìš”\s*ë‰´ìŠ¤ì…ë‹ˆë‹¤", text)[0]

    # 4) URL ì œê±°
    url_patterns = [
        r"https?://\S+",
        r"pic\.twitter\.com/\S+",
        r"@[A-Za-z0-9_]+",
    ]
    for p in url_patterns:
        text = re.sub(p, "", text)

    # 5) ë”°ì˜´í‘œ ì •ë¦¬
    text = text.replace("â€œ", "\"").replace("â€", "\"")
    text = text.replace("â€˜", "'").replace("â€™", "'")
    text = re.sub(r'""+', '"', text)

    # 6) ë¬¸ì¥ ë¶„ë¦¬
    text = re.sub(
        r"(?<=[.!?])\s*(?=[ê°€-í£A-Za-z0-9])",
        "\n",
        text
    )

    # 7) ê¸°íƒ€ ì œê±°
    garbage = ["ì €ì‘ê¶Œì Â©", "ë¬´ë‹¨ì „ì¬ ë° ì¬ë°°í¬ ê¸ˆì§€", "ê´€ë ¨ê¸°ì‚¬", "ì¶”ì²œê¸°ì‚¬", "â–³"]
    for g in garbage:
        text = text.replace(g, "")

    # 8) ê³µë°± ì •ë¦¬
    text = re.sub(r"\s{2,}", " ", text)
    text = re.sub(r"\n\s+", "\n", text)

    # 9) ë§ˆì§€ë§‰ ë¼ì¸ ê¸°ìëª… ì œê±°
    text = re.sub(r"\n?[ê°€-í£]{2,3}\s*$", "", text).strip()

    return text.strip()


# ---------------------------------------------------------
# ë³¸ë¬¸ ì—†ëŠ” ê¸°ì‚¬ ì œì™¸ ì¡°ê±´
# ---------------------------------------------------------
def is_valid_article_content(text):
    if not text:
        return False
    if len(text) < 40:
        return False
    sentences = [s for s in text.split("\n") if s.strip()]
    if len(sentences) < 2:
        return False
    hangul = len(re.findall(r"[ê°€-í£]", text))
    if hangul / max(len(text), 1) < 0.05:
        return False
    return True


# ---------------------------------------------------------
# ê¸°ì‚¬ URL ìˆ˜ì§‘
# ---------------------------------------------------------
def get_article_urls(target_count=20):
    urls = []
    page = 1

    while len(urls) < target_count:
        list_url = f"{BASE_URL}/news/articleList.html?view_type=sm&page={page}"
        res = requests.get(list_url, headers={"User-Agent": "Mozilla/5.0"})
        soup = BeautifulSoup(res.text, "html.parser")

        for a in soup.select(".altlist-subject a"):
            href = a.get("href")
            if href and "articleView" in href:
                urls.append(urljoin(BASE_URL, href))

        print(f"ğŸ“„ í˜ì´ì§€ {page} â†’ ëˆ„ì  {len(urls)}ê°œ URL ìˆ˜ì§‘ë¨")
        page += 1

    return urls[:target_count]


# ---------------------------------------------------------
# ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
# ---------------------------------------------------------
def parse_article(url):
    res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
    soup = BeautifulSoup(res.text, "html.parser")

    category = clean_invisible_chars(
        soup.select_one(".section").get_text(strip=True)
    ) if soup.select_one(".section") else ""

    title_raw = clean_invisible_chars(
        soup.select_one(".heading").get_text(strip=True)
    ) if soup.select_one(".heading") else ""
    title = clean_title(title_raw)

    date_tag = soup.select_one(".breadcrumbs li:nth-child(2)")
    article_date = extract_korean_date(date_tag.get_text(strip=True)) if date_tag else None

    body_tag = soup.select_one("#article-view-content-div")
    content_raw = clean_invisible_chars(
        body_tag.get_text(" ", strip=True)
    ) if body_tag else ""
    content = clean_article_body(content_raw)

    if not is_valid_article_content(content):
        print(f"âš ï¸ ë³¸ë¬¸ ì—†ìŒ â†’ ì œì™¸: {url}")
        return None

    return {
        "url": url,
        "category": category,
        "title": title,
        "content": content,
        "article_date": article_date.strftime("%Y-%m-%d %H:%M") if article_date else "",
        "source": "AITimes"
    }


# ---------------------------------------------------------
# ì‹¤í–‰
# ---------------------------------------------------------
if __name__ == "__main__":
    urls = get_article_urls(target_count=120)

    articles = []

    print("\n==================== í¬ë¡¤ë§ ì‹œì‘ ====================\n")

    for u in urls:
        print(f"ğŸ” í¬ë¡¤ë§: {u}")
        art = parse_article(u)
        if art:
            articles.append(art)
            pretty_print_article(art)
